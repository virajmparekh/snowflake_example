{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Astronomer Airflow with Snowflake \n",
    "\n",
    "### Prerequisites\n",
    "1) A valid Snowflake and S3 account\n",
    "\n",
    "2) The Astronomer CLI or a running version of Airflow. (This guide was written to work with Airflow on Astronomer, but the same code should work for vanilla Airflow as well) \n",
    "\n",
    "Navigate here to get set up:\n",
    "https://github.com/astronomerio/astro-cli\n",
    "\n",
    " \n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Navigate to a project directory and run `astro airflow init` in a terminal.\n",
    "\n",
    "This will generate a skeleton file directory:\n",
    "```\n",
    ".\n",
    "├── dags\n",
    "│   └── example-dag.py\n",
    "├── Dockerfile\n",
    "├── include\n",
    "├── packages.txt\n",
    "├── plugins\n",
    "\n",
    "```\n",
    "Clone this repository into your plugins folder:\n",
    "https://github.com/airflow-plugins/snowflake_plugin\n",
    "\n",
    "This gives you the Airflow plugins needed to interact with Snowflake. For a full list of community contributed plugins, check out:\n",
    "- https://github.com/apache/incubator-airflow/tree/master/airflow\n",
    "\n",
    "- https://github.com/airflow-plugins/\n",
    "\n",
    "\n",
    "### Start a local Airflow Instance:\n",
    "\n",
    "Before you can spin up Airflow, you will need to specify that your image builds with all of the dependencies necessary to snowflake and Amazon S3. \n",
    "\n",
    "Add the following to your `packages.txt` and `requirements.txt`:\n",
    "\n",
    "packages.txt\n",
    "    \n",
    "    musl\n",
    "    gcc\n",
    "    make\n",
    "    g++\n",
    "    lz4-dev\n",
    "    cyrus-sasl-dev\n",
    "    openssl-dev\n",
    "    python3-dev\n",
    "    \n",
    "requirements.txt\n",
    "    \n",
    "    azure-common==1.1.14\n",
    "    azure-nspkg==2.0.0\n",
    "    azure-storage==0.36.0\n",
    "    ijson==2.3\n",
    "    pycryptodome==3.6.4\n",
    "    snowflake-connector-python==1.6.5\n",
    "    \n",
    "\n",
    "<br>\n",
    "Now run\n",
    "\n",
    "    `astro airflow start`\n",
    "<br>\n",
    "\n",
    "This should spin up a few docker containers on your machine. Run `docker ps` and you should see:\n",
    "\n",
    "```\n",
    "CONTAINER ID        IMAGE                     COMMAND                  CREATED             STATUS              PORTS                                        NAMES\n",
    "1fc88586da10        notebook/airflow:latest   \"tini -- /entrypoint…\"   5 seconds ago       Up 5 seconds        5555/tcp, 8793/tcp, 0.0.0.0:8080->8080/tcp   notebook_webserver_1\n",
    "a1d02ea75c2b        notebook/airflow:latest   \"tini -- /entrypoint…\"   6 seconds ago       Up 1 second         5555/tcp, 8080/tcp, 8793/tcp                 notebook_scheduler_1\n",
    "d0edb1f6c497        postgres:10.1-alpine      \"docker-entrypoint.s…\"   6 seconds ago       Up 6 seconds        0.0.0.0:5432->5432/tcp                       notebook_postgres_1\n",
    "```\n",
    "\n",
    "###  Enter your Connection Credentials\n",
    "\n",
    "Navigate to Admin-->Connections-->Create and create a new connection within your Airflow instance. The `conn_id` will be used to refer to your connection.\n",
    "\n",
    "This will be your local development environment. Navigate to `localhost:8080` to see your Airflow dashboard.\n",
    "<br> ![connection](img/snowflake_conn.png)\n",
    "<br>\n",
    "\n",
    "Do the same thing for your S3 connection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Write your DAG\n",
    "\n",
    "Because the `snowflake_plugin` was added to the `plugins` directory, it can be imported as an airflow plugin.\n",
    "\n",
    "See the attached example for what this could look like. DAG files should go in the `dags` folder.\n",
    "\n",
    "\n",
    "### Deploy your DAG\n",
    "\n",
    "Once you get your DAG working locally and your Astronomer cluster deployed, you can authenticate and start deploying!\n",
    "\n",
    "Run \n",
    "\n",
    "    astro auth login\n",
    "\n",
    "You should be prompted to log into your instance. Once you've authenticated, run\n",
    "\n",
    "    astro airflow deploy\n",
    "    \n",
    "and chose which Airflow instance you want to deploy to.\n",
    "\n",
    "The deploy will packages the entire project directory (dags, plugins, and all the requirements and packages needed for the code to run) into a Docker image and push it to the your Kubernetes Cluster.\n",
    "\n",
    "Once you enter all your credentials in your production instance, everything is good to go.\n",
    "\n",
    "![dag_success](img/snowflake_dag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
